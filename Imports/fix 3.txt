Agent Zero Vault – Implementation Guide for Google AI Studio
This document summarises the new code additions and modifications designed to bring the Agent Zero Vault up to date with the backlog items. You can pass these instructions directly into Google AI Studio or your codebase to implement the changes.

1. Create a FastAPI backend (AZV‑020)
Files to create or replace
backend/app/main.py – the entrypoint for the API:

python
Copy
Edit
"""
Agent Zero Vault API
====================

This FastAPI application provides the backend for the Agent Zero Vault system.  It implements core services such as health checks and a simple in‑memory Task queue used by the Task & Review Hub.  In a production deployment this module would connect to a real database and incorporate authentication and data persistence.  For the purposes of the initial implementation, tasks are stored in a process‑local list.

Endpoints
---------

* `GET /api/v1/health` – return basic health status of the API
* `GET /api/v1/tasks` – list all tasks ordered newest first
* `POST /api/v1/tasks` – create a new task
* `PUT /api/v1/tasks/{task_id}/resolve` – mark a task as resolved

Run the application with: `uvicorn app.main:app --reload`
"""

from __future__ import annotations

import uuid
from datetime import datetime
from fastapi import FastAPI, HTTPException, status
from pydantic import BaseModel, Field

app = FastAPI(title="Agent Zero Vault API", version="0.1.0")

class TaskCreateRequest(BaseModel):
    source: str = Field(..., description="Source app creating the task")
    sourceId: str = Field(..., description="Unique identifier for the source event")
    title: str = Field(..., description="Brief summary of the task")
    description: str = Field(..., description="Detailed description of the task")
    priority: str = Field(..., description="Task priority: Low, Medium, High, Critical")
    appId: str = Field(..., description="Identifier of the app to open for resolving the task")

class TaskItem(BaseModel):
    id: str
    source: str
    sourceId: str
    title: str
    description: str
    priority: str
    status: str
    timestamp: str
    appId: str

# In‑memory task list.  Replace with a database for production.
_tasks: list[TaskItem] = []

@app.get("/api/v1/health")
async def health_check() -> dict[str, str]:
    return {"status": "ok"}

@app.get("/api/v1/tasks", response_model=list[TaskItem])
async def list_tasks() -> list[TaskItem]:
    return sorted(_tasks, key=lambda t: t.timestamp, reverse=True)

@app.post("/api/v1/tasks", response_model=TaskItem, status_code=status.HTTP_201_CREATED)
async def create_task(task: TaskCreateRequest) -> TaskItem:
    # Prevent duplicate pending tasks by sourceId
    for existing in _tasks:
        if existing.sourceId == task.sourceId and existing.status == "Pending":
            raise HTTPException(status_code=409, detail="Duplicate task for this sourceId")
    new_task = TaskItem(
        id=str(uuid.uuid4()),
        source=task.source,
        sourceId=task.sourceId,
        title=task.title,
        description=task.description,
        priority=task.priority,
        status="Pending",
        timestamp=datetime.utcnow().isoformat(),
        appId=task.appId,
    )
    _tasks.append(new_task)
    return new_task

@app.put("/api/v1/tasks/{task_id}/resolve", response_model=TaskItem)
async def resolve_task(task_id: str) -> TaskItem:
    for idx, t in enumerate(_tasks):
        if t.id == task_id:
            if t.status == "Resolved":
                raise HTTPException(status_code=400, detail="Task already resolved")
            updated = t.copy(update={"status": "Resolved"})
            _tasks[idx] = updated
            return updated
    raise HTTPException(status_code=404, detail="Task not found")
backend/Dockerfile – containerises the API:

dockerfile
Copy
Edit
FROM python:3.11-slim

# Install system dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /app

# Install Python dependencies
COPY backend/requirements.txt /tmp/requirements.txt
RUN pip install --no-cache-dir -r /tmp/requirements.txt

# Copy application code
COPY backend/app /app/app

EXPOSE 8000

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
docker-compose.yml – orchestrates the API and a PostgreSQL database:

yaml
Copy
Edit
auto_version: 3.8

services:
  api:
    build:
      context: .
      dockerfile: backend/Dockerfile
    env_file:
      - .env.local
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app/backend
    depends_on:
      - db

  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: azv
      POSTGRES_USER: azv_user
      POSTGRES_PASSWORD: azv_pass
    ports:
      - "5432:5432"
    volumes:
      - db_data:/var/lib/postgresql/data

volumes:
  db_data:
    driver: local
You can run the backend locally via docker-compose up --build and it will be available on http://localhost:8000.

2. Add a Global Event Bus (AZV‑021)
2.1 Define the Event Map
In types.ts, below the existing TaskItem definition, add a typed map of events and payloads:

ts
Copy
Edit
export interface EventMap {
  /** Emitted whenever a new task is created via `taskQueueService.addTask`. */
  'task.created': TaskItem;
  /** Emitted when a task is resolved. */
  'task.resolved': { id: string };
  /** General system alert payload. */
  'system.alert': { level: LogLevel; message: string };
  // Extend this map with further events as needed.
}
2.2 Implement the Event Bus
Create services/eventBus.ts containing a strongly typed publish–subscribe service:

ts
Copy
Edit
import type { EventMap } from '../types';

// Generic callback based on event name and payload
type EventCallback<E extends keyof EventMap> = (payload: EventMap[E]) => void;

class EventBus {
  private subscribers: { [K in keyof EventMap]?: Set<EventCallback<K>> } = {};

  publish<E extends keyof EventMap>(event: E, payload: EventMap[E]): void {
    const handlers = this.subscribers[event];
    if (!handlers) return;
    [...handlers].forEach(callback => {
      try {
        callback(payload);
      } catch (err) {
        console.error(`Error in handler for ${String(event)}:`, err);
      }
    });
  }

  subscribe<E extends keyof EventMap>(event: E, callback: EventCallback<E>): () => void {
    if (!this.subscribers[event]) {
      this.subscribers[event] = new Set();
    }
    (this.subscribers[event] as Set<EventCallback<E>>).add(callback);
    return () => {
      (this.subscribers[event] as Set<EventCallback<E>>)?.delete(callback);
    };
  }
}

// Export a singleton instance of the bus
export const eventBus = new EventBus();
2.3 Wire the bus into the Task Queue service
In services/taskQueueService.ts, import and publish events whenever tasks are created or resolved:

ts
Copy
Edit
import { eventBus } from './eventBus';

// After successfully posting a new task
const created: TaskItem = await response.json();
eventBus.publish('task.created', created);

// When resolving a task
eventBus.publish('task.resolved', { id: taskId });
These events can be subscribed to from any component or service, decoupling inter‑app communication.

3. Running the System
Ensure the .env.local file contains a GEMINI_API_KEY (the modifications in services/geminiClient.ts now look for either API_KEY or GEMINI_API_KEY).

Use docker-compose up --build to start the backend and database.

Run the frontend with npm install && npm run dev from the project root. The Task & Review Hub will now call the FastAPI endpoints, and events will be published via the new event bus.

4. Next Steps
These changes lay the foundation for future backlog items, but several additional features have now been implemented and require integration:

Persist tasks to PostgreSQL or another database using SQLAlchemy or an ORM.

Refactor existing services (e.g. controlPanelService, loomService) to publish and subscribe to events via the bus. Portions of this work are already completed in this guide.

Add more events to the EventMap as new features (e.g. notifications, ingestion progress) are implemented.

Implement component rendering optimisations (AZV‑034) to prevent unnecessary React re-renders.

5. Implement Save Functionality in the System Editor (AZV‑023)
The System Editor now allows users to modify files and persist their changes back into the in‑memory file tree. The following updates were made:

File System Service – added a saveFileContent method in services/fileSystemService.ts to update a file node’s content field:

ts
Copy
Edit
public saveFileContent(path: string, newContent: string): boolean {
  const parts = path.split('/').filter(Boolean);
  let current: FileSystemNode | undefined = this.root;
  for (const part of parts) {
    if (!current?.children) return false;
    current = current.children.find(c => c.name === part);
  }
  if (current && typeof current.content === 'string') {
    current.content = newContent;
    return true;
  }
  return false;
}
Editor component changes – components/editor/MonacoWrapper.tsx now accepts an onChange callback and registers a onDidChangeModelContent listener so parent components are notified whenever the text changes. EditorPane.tsx forwards these events to the parent and displays a Save button on the tab bar. When clicked the save handler persists the active file:

tsx
Copy
Edit
<MonacoWrapper
  path={activeFile.path}
  defaultValue={activeFile.content}
  onChange={(val) => onContentChange?.(activeFile.path, val)}
/>;

// In the tab bar
{activeFile && onSave && (
  <button onClick={() => onSave(activeFile.path)} className="...">Save</button>
)}
SystemEditorApp modifications – the app tracks edits in its openFiles state and exposes handleContentChange and handleSaveActiveFile. A console message is appended on success or failure:

tsx
Copy
Edit
const handleContentChange = useCallback((path: string, newContent: string) => {
  setOpenFiles(prev => prev.map(f => (f.path === path ? { ...f, content: newContent } : f)));
}, []);

const handleSaveActiveFile = useCallback((path: string) => {
  const file = openFiles.find(f => f.path === path);
  if (!file) return;
  const ok = fileSystemService.saveFileContent(path, file.content);
  setConsoleOutput(prev => [...prev, ok ? `[EDITOR] Saved ${file.name}` : `[EDITOR] Failed to save ${file.name}`]);
}, [openFiles]);
With these changes, users can open, edit and save files within the System Editor without leaving the browser.

6. Define a Data Contract for the Automation Hub (AZV‑024)
Workflows triggered by other apps now use a structured input and output contract to exchange data. This avoids the ambiguous context: any field and makes results easier to consume.

6.1 Update the types
In types.ts, add WorkflowInput and WorkflowOutput definitions and extend WorkflowRun:

ts
Copy
Edit
export interface WorkflowInput {
  sourceApp: string;
  payload: any;
}

export interface WorkflowOutput {
  success: boolean;
  data?: any;
  error?: string;
}

export interface WorkflowRun {
  id: string;
  timestamp: string;
  status: 'Success' | 'Failed' | 'Running';
  durationMs: number;
  input?: WorkflowInput;
  output?: WorkflowOutput;
}
6.2 Update the N8N service
Refactor services/n8nService.ts to accept this new contract:

ts
Copy
Edit
runWorkflow(workflowId: string) {
  this.runWorkflowWithInput(workflowId, undefined);
}

runWorkflowWithInput(workflowId: string, input: WorkflowInput | undefined) {
  // create a WorkflowRun with `input`
  const run: WorkflowRun = {
    id: `run-${this.runIdCounter++}`,
    timestamp: new Date().toLocaleString(),
    status: 'Running',
    durationMs: 0,
    input,
  };
  ...
  // when the run completes, build a `WorkflowOutput` and assign it
  run.output = success ? { success: true, data: { message: ... } } : { success: false, error: ... };
}
6.3 Update callers
Apps that trigger workflows should now pass a WorkflowInput. For example, CuratorApp.tsx uses the new method when creating Shopify placeholders:

ts
Copy
Edit
n8nService.runWorkflowWithInput('wf-create-placeholders', {
  sourceApp: 'Curator',
  payload: { isbns: selectedBookIsbns },
});
These changes ensure that workflows are invoked with contextual data and return a predictable structure.

7. Introduce a Request Queue for Gemini API Calls (AZV‑033)
To prevent excessive concurrent calls to the Google GenAI API and reduce cost, a simple concurrency queue has been added to services/geminiClient.ts. A maximum of two requests will run at the same time; additional requests are queued and executed in order.

7.1 Queue implementation
ts
Copy
Edit
interface QueuedRequest<T> {
  fn: () => Promise<T>;
  resolve: (value: T | PromiseLike<T>) => void;
  reject: (reason?: any) => void;
}

const MAX_CONCURRENT_REQUESTS = 2;
let activeRequests = 0;
const pendingQueue: QueuedRequest<unknown>[] = [];

async function dequeueAndRun() {
  if (activeRequests >= MAX_CONCURRENT_REQUESTS || pendingQueue.length === 0) return;
  const { fn, resolve, reject } = pendingQueue.shift()!;
  activeRequests++;
  try {
    resolve(await fn());
  } catch (err) {
    reject(err);
  } finally {
    activeRequests--;
    setTimeout(dequeueAndRun, 0);
  }
}

function enqueueRequest<T>(fn: () => Promise<T>): Promise<T> {
  return new Promise((resolve, reject) => {
    pendingQueue.push({ fn, resolve, reject });
    dequeueAndRun();
  });
}
7.2 Wrapping API calls
The generateText and generateImageFromPrompt functions are now executed through enqueueRequest:

ts
Copy
Edit
export const generateText = async (...) => {
  const doRequest = async () => {
    const response = await ai.models.generateContent({ ... });
    ...
  };
  return await enqueueRequest(doRequest);
};

export const generateImageFromPrompt = async (prompt: string): Promise<string> => {
  const doRequest = async () => {
    const response = await ai.models.generateImages({ ... });
    ...
  };
  return await enqueueRequest(doRequest);
};
With this queue in place the system will honour a maximum concurrency level across all Gemini requests, improving stability and cost control.

Summary
This document now covers both the initial backend/event bus foundations and additional backlog implementations including editor save support, a structured Automation Hub contract, and basic request batching for Gemini API calls. Integrate these snippets into your codebase and continue iterating on the remaining backlog stories.